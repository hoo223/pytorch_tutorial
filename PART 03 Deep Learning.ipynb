{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 3-1] dropout 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bce42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc365d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.11.0+cu113  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활요하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5a6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bc309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. MNIST 데이터 다운로드(Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"./data/MNIST\",\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root = \"./data/MNIST/\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774d04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3319c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16cc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train() # 모델을 학습 상태로 지정\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "        label = label.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "        optimizer.zero_grad() # 기존 할당되어 있던 gradient 값 초기화\n",
    "        output = model(image) # Forward propagation\n",
    "        loss = criterion(output, label) # loss 계산\n",
    "        loss.backward() # Backpropagation을 통해 계산된 gradient 값을 각 파라미터에 할당\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                  Epoch, batch_idx * len(image),\n",
    "                  len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                  loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02905730",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval() # 모델을 평가 상태로 지정\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad(): # 평가하는 단계에서 gradient를 통해 파라미터 값이 업데이트되는 현상을 방지\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "            label = label.to(DEVICE) # 기존 정의한 장비에 할당\n",
    "            output = model(image) # Forward propagation\n",
    "            test_loss += criterion(output, label).item() # loss 누적\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # 평균 loss 계산\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71da9cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_with_dropout(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.296960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 2.289205\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 2.316490\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 2.314373\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 2.412419\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 2.303002\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 2.286214\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 2.340079\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 2.387946\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 2.256034\n",
      "\n",
      "EPOCH: 1], \tTest Loss: 0.0711, \tTest Accuracy: 21.52 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 2.376101\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 2.266825\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 2.278690\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 2.291783\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 2.220051\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 2.315132\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 2.209401\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 2.285427\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 2.132220\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 2.096918\n",
      "\n",
      "EPOCH: 2], \tTest Loss: 0.0625, \tTest Accuracy: 34.55 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 1.901922\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 2.176054\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 2.036504\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 1.823882\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 1.694908\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 1.480134\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 1.498440\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 1.464749\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 1.419463\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 1.209394\n",
      "\n",
      "EPOCH: 3], \tTest Loss: 0.0384, \tTest Accuracy: 59.21 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 1.720856\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 1.321560\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 1.397621\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 1.194909\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 1.423320\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 1.313674\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 1.082034\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.938071\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 1.012583\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 1.067495\n",
      "\n",
      "EPOCH: 4], \tTest Loss: 0.0280, \tTest Accuracy: 70.41 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.710595\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.720616\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.866386\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.967170\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 1.204113\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.812179\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.803840\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.728187\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.702451\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 1.316314\n",
      "\n",
      "EPOCH: 5], \tTest Loss: 0.0240, \tTest Accuracy: 75.68 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.891841\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 1.096103\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 1.017413\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.610996\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 1.016693\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.999794\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 1.033319\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.546681\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.588240\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.878745\n",
      "\n",
      "EPOCH: 6], \tTest Loss: 0.0208, \tTest Accuracy: 79.75 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.694946\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.984555\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.613043\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.721867\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.875382\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.742373\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.662272\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.605121\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.767500\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.637756\n",
      "\n",
      "EPOCH: 7], \tTest Loss: 0.0184, \tTest Accuracy: 82.39 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.474207\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.900006\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.539645\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.572639\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 1.078089\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.462199\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.466211\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.592264\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.569577\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.628159\n",
      "\n",
      "EPOCH: 8], \tTest Loss: 0.0165, \tTest Accuracy: 84.59 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.603857\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.634147\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.751658\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.511412\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.364394\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.767544\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.461017\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.733842\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.713741\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.562831\n",
      "\n",
      "EPOCH: 9], \tTest Loss: 0.0151, \tTest Accuracy: 85.93 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.532292\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.585002\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.949226\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.534067\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.401748\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.550245\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.747788\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.626632\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.545034\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.417743\n",
      "\n",
      "EPOCH: 10], \tTest Loss: 0.0143, \tTest Accuracy: 86.70 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net_with_dropout(nn.Module): # nn.Module 클래스 상속 -> 딥러닝 모델 관련 기본 함수를 포함\n",
    "    def __init__(self): #  인스턴스 생성 시 지니게 되는 성질을 정의해주는 메서드\n",
    "        super(Net_with_dropout, self).__init__() # nn.Module 내에 있는 매서드를 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256) # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10) # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5\n",
    "    def forward(self, x): # Forward Propagation 정의\n",
    "        x = x.view(-1, 28 * 28) # 2차원 데이터를 1차원 데이터로 변환 -> MLP 모델은 1차원의 벡터 값을 입력으로 받을 수 있음\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x) # 활성화 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) # 확률 값 계산. log_softmax를 사용하는 이유는 gradient 값을 좀 더 원활하게 계산하기 위해 \n",
    "        return x\n",
    "\n",
    "model = Net_with_dropout().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\nEPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b816907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 3-2] dropout + relu 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2be8c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_with_dropout_relu(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.314856\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 2.071999\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 1.242158\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.716584\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.603665\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.512974\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.442335\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.469546\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.573612\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.270458\n",
      "\n",
      "EPOCH: 1], \tTest Loss: 0.0104, \tTest Accuracy: 90.49 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.433970\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.283557\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.323988\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.287774\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.275695\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.424920\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.235228\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.315157\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.415996\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.205878\n",
      "\n",
      "EPOCH: 2], \tTest Loss: 0.0072, \tTest Accuracy: 93.27 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.130904\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.455021\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.315734\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.431916\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.450832\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.165035\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.545182\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.127807\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.204825\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.228863\n",
      "\n",
      "EPOCH: 3], \tTest Loss: 0.0056, \tTest Accuracy: 94.85 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.185883\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.058677\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.155023\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.117300\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.177463\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.371362\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.116708\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.108601\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.267276\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.184085\n",
      "\n",
      "EPOCH: 4], \tTest Loss: 0.0045, \tTest Accuracy: 95.67 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.204336\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.211351\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.245947\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.322370\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.072620\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.086519\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.162023\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.178584\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.121114\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.225071\n",
      "\n",
      "EPOCH: 5], \tTest Loss: 0.0038, \tTest Accuracy: 96.39 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.119625\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.114069\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.099913\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.314146\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.435746\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.080438\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.187732\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.078194\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.330878\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.114678\n",
      "\n",
      "EPOCH: 6], \tTest Loss: 0.0033, \tTest Accuracy: 96.83 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.269174\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.104317\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.154303\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.202613\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.039274\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.266998\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.170836\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.061098\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.053750\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.034264\n",
      "\n",
      "EPOCH: 7], \tTest Loss: 0.0029, \tTest Accuracy: 97.28 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.052232\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.032371\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.073101\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.164271\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.090774\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.027853\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.071261\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.010523\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.100487\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.164006\n",
      "\n",
      "EPOCH: 8], \tTest Loss: 0.0026, \tTest Accuracy: 97.62 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.143701\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.079091\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.193726\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.046470\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.028533\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.114958\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.473487\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.056374\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.019072\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.018123\n",
      "\n",
      "EPOCH: 9], \tTest Loss: 0.0023, \tTest Accuracy: 97.83 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.054258\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.059672\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.051272\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.150944\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.091822\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.045982\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.027577\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.184902\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.107456\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.086032\n",
      "\n",
      "EPOCH: 10], \tTest Loss: 0.0022, \tTest Accuracy: 97.93 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net_with_dropout_relu(nn.Module): # nn.Module 클래스 상속 -> 딥러닝 모델 관련 기본 함수를 포함\n",
    "    def __init__(self): #  인스턴스 생성 시 지니게 되는 성질을 정의해주는 메서드\n",
    "        super(Net_with_dropout_relu, self).__init__() # nn.Module 내에 있는 매서드를 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256) # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10) # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5\n",
    "    def forward(self, x): # Forward Propagation 정의\n",
    "        x = x.view(-1, 28 * 28) # 2차원 데이터를 1차원 데이터로 변환 -> MLP 모델은 1차원의 벡터 값을 입력으로 받을 수 있음\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x) # 활성화 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) # 확률 값 계산. log_softmax를 사용하는 이유는 gradient 값을 좀 더 원활하게 계산하기 위해 \n",
    "        return x\n",
    "    \n",
    "model = Net_with_dropout_relu().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\nEPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6771a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 3-3] dropout + relu + Batch Normalization 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f93a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_with_dropout_relu_BN(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.435903\n",
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 0.654234\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.729689\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.214407\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.340866\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.354225\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.434620\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.312786\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.233698\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.117500\n",
      "\n",
      "EPOCH: 1], \tTest Loss: 0.0047, \tTest Accuracy: 95.47 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.111339\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.273056\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.038755\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.311179\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.319129\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.161401\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.550950\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.209684\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.287426\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.297424\n",
      "\n",
      "EPOCH: 2], \tTest Loss: 0.0034, \tTest Accuracy: 96.71 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.333308\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.408498\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.179832\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.209408\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.097394\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.046375\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.331062\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.103442\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.199186\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.100582\n",
      "\n",
      "EPOCH: 3], \tTest Loss: 0.0027, \tTest Accuracy: 97.41 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.104386\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.240378\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.214016\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.044371\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.222248\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.106591\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.258541\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.280809\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.195656\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.321302\n",
      "\n",
      "EPOCH: 4], \tTest Loss: 0.0022, \tTest Accuracy: 98.00 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.122103\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.252062\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.139274\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.207005\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.491799\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.264777\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.459687\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.104906\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.008957\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.079451\n",
      "\n",
      "EPOCH: 5], \tTest Loss: 0.0020, \tTest Accuracy: 98.11 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.061224\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.083592\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.074338\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.081702\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.112474\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.057491\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.431410\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.050827\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.131478\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.296092\n",
      "\n",
      "EPOCH: 6], \tTest Loss: 0.0018, \tTest Accuracy: 98.27 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.083139\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.237678\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.161018\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.120678\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.352172\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.096063\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.201042\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.059492\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.167780\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.123494\n",
      "\n",
      "EPOCH: 7], \tTest Loss: 0.0017, \tTest Accuracy: 98.42 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.169265\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.165405\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.202523\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.026020\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.267106\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.292982\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.108661\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.041757\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.346109\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.103931\n",
      "\n",
      "EPOCH: 8], \tTest Loss: 0.0015, \tTest Accuracy: 98.54 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.113981\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.119542\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.250481\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.013244\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.033104\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.205214\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.052349\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.016802\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.443430\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.086960\n",
      "\n",
      "EPOCH: 9], \tTest Loss: 0.0014, \tTest Accuracy: 98.67 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.142236\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.064001\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.135798\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.042919\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.383602\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.024606\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.101073\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.072276\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.028964\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.102530\n",
      "\n",
      "EPOCH: 10], \tTest Loss: 0.0012, \tTest Accuracy: 98.83 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net_with_dropout_relu_BN(nn.Module): # nn.Module 클래스 상속 -> 딥러닝 모델 관련 기본 함수를 포함\n",
    "    def __init__(self): #  인스턴스 생성 시 지니게 되는 성질을 정의해주는 메서드\n",
    "        super(Net_with_dropout_relu_BN, self).__init__() # nn.Module 내에 있는 매서드를 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256) # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10) # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # 첫 번째 FC layer의 output 크기와 동일하게\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # 두 번째 FC layer의 output 크기와 동일하게\n",
    "    def forward(self, x): # Forward Propagation 정의\n",
    "        x = x.view(-1, 28 * 28) # 2차원 데이터를 1차원 데이터로 변환 -> MLP 모델은 1차원의 벡터 값을 입력으로 받을 수 있음\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x) # 활성화 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) # 확률 값 계산. log_softmax를 사용하는 이유는 gradient 값을 좀 더 원활하게 계산하기 위해 \n",
    "        return x\n",
    "    \n",
    "model = Net_with_dropout_relu_BN().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\nEPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7725807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 3-4] dropout + relu + Batch Normalization + He Uniform Initialization 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b703f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_with_dropout_relu(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.794667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27398/569612395.py:27: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data) # 파라미터 값 초기화\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 1.041914\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.721174\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.514280\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.585400\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.579763\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.585719\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.473004\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.663243\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.513607\n",
      "\n",
      "EPOCH: 1], \tTest Loss: 0.0080, \tTest Accuracy: 92.53 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.391760\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.430022\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.613833\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.392684\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.470594\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.489169\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.199942\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.701211\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.624363\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.178584\n",
      "\n",
      "EPOCH: 2], \tTest Loss: 0.0059, \tTest Accuracy: 94.42 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.696755\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.248364\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.401911\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.420872\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.307351\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.234097\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.397402\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.534006\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.284952\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.203645\n",
      "\n",
      "EPOCH: 3], \tTest Loss: 0.0048, \tTest Accuracy: 95.40 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.224333\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.227411\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.475128\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.322496\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.118834\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.150747\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.182600\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.150962\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.393320\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.273272\n",
      "\n",
      "EPOCH: 4], \tTest Loss: 0.0040, \tTest Accuracy: 96.17 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.074778\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.096164\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.226767\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.240439\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.090097\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.168943\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.092409\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.236555\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.353929\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.005207\n",
      "\n",
      "EPOCH: 5], \tTest Loss: 0.0035, \tTest Accuracy: 96.67 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.178991\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.303862\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.315060\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.129964\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.213263\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.247435\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.059749\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.035646\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.126836\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.237111\n",
      "\n",
      "EPOCH: 6], \tTest Loss: 0.0031, \tTest Accuracy: 97.08 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.398635\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.225577\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.262820\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.115651\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.357159\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.274498\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.305207\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.418861\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.214080\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.114417\n",
      "\n",
      "EPOCH: 7], \tTest Loss: 0.0028, \tTest Accuracy: 97.36 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.158307\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.271472\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.045105\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.104147\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.115062\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.078483\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.146101\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.180411\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.150847\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.063464\n",
      "\n",
      "EPOCH: 8], \tTest Loss: 0.0025, \tTest Accuracy: 97.57 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.276796\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.128810\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.103739\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.018530\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.259012\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.085395\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.232619\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.154509\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.124461\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.059605\n",
      "\n",
      "EPOCH: 9], \tTest Loss: 0.0023, \tTest Accuracy: 97.81 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.201471\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.183424\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.083343\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.026860\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.048355\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.094940\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.026192\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.105783\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.139703\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.060407\n",
      "\n",
      "EPOCH: 10], \tTest Loss: 0.0021, \tTest Accuracy: 98.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net_with_dropout_relu_BN(nn.Module): # nn.Module 클래스 상속 -> 딥러닝 모델 관련 기본 함수를 포함\n",
    "    def __init__(self): #  인스턴스 생성 시 지니게 되는 성질을 정의해주는 메서드\n",
    "        super(Net_with_dropout_relu, self).__init__() # nn.Module 내에 있는 매서드를 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256) # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10) # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # 첫 번째 FC layer의 output 크기와 동일하게\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # 두 번째 FC layer의 output 크기와 동일하게\n",
    "    def forward(self, x): # Forward Propagation 정의\n",
    "        x = x.view(-1, 28 * 28) # 2차원 데이터를 1차원 데이터로 변환 -> MLP 모델은 1차원의 벡터 값을 입력으로 받을 수 있음\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x) # 활성화 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) # 확률 값 계산. log_softmax를 사용하는 이유는 gradient 값을 좀 더 원활하게 계산하기 위해 \n",
    "        return x\n",
    "\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear): # MLP 모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "        init.kaiming_uniform(m.weight.data) # 파라미터 값 초기화\n",
    "\n",
    "model = Net_with_dropout_relu().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\nEPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a866d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 3-5] dropout + relu + Batch Normalization + He Uniform Initialization + Adam 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c637f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net_with_dropout_relu_BN(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000(0%)]\tTrain Loss: 2.994092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27398/3987426498.py:27: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data) # 파라미터 값 초기화\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000(11%)]\tTrain Loss: 0.594844\n",
      "Train Epoch: 1 [12800/60000(21%)]\tTrain Loss: 0.335959\n",
      "Train Epoch: 1 [19200/60000(32%)]\tTrain Loss: 0.259938\n",
      "Train Epoch: 1 [25600/60000(43%)]\tTrain Loss: 0.413181\n",
      "Train Epoch: 1 [32000/60000(53%)]\tTrain Loss: 0.145232\n",
      "Train Epoch: 1 [38400/60000(64%)]\tTrain Loss: 0.137868\n",
      "Train Epoch: 1 [44800/60000(75%)]\tTrain Loss: 0.442964\n",
      "Train Epoch: 1 [51200/60000(85%)]\tTrain Loss: 0.324795\n",
      "Train Epoch: 1 [57600/60000(96%)]\tTrain Loss: 0.403449\n",
      "\n",
      "EPOCH: 1], \tTest Loss: 0.0041, \tTest Accuracy: 96.01 %\n",
      "\n",
      "Train Epoch: 2 [0/60000(0%)]\tTrain Loss: 0.272545\n",
      "Train Epoch: 2 [6400/60000(11%)]\tTrain Loss: 0.084270\n",
      "Train Epoch: 2 [12800/60000(21%)]\tTrain Loss: 0.138753\n",
      "Train Epoch: 2 [19200/60000(32%)]\tTrain Loss: 0.071402\n",
      "Train Epoch: 2 [25600/60000(43%)]\tTrain Loss: 0.156637\n",
      "Train Epoch: 2 [32000/60000(53%)]\tTrain Loss: 0.502050\n",
      "Train Epoch: 2 [38400/60000(64%)]\tTrain Loss: 0.347759\n",
      "Train Epoch: 2 [44800/60000(75%)]\tTrain Loss: 0.155971\n",
      "Train Epoch: 2 [51200/60000(85%)]\tTrain Loss: 0.231930\n",
      "Train Epoch: 2 [57600/60000(96%)]\tTrain Loss: 0.035915\n",
      "\n",
      "EPOCH: 2], \tTest Loss: 0.0027, \tTest Accuracy: 97.33 %\n",
      "\n",
      "Train Epoch: 3 [0/60000(0%)]\tTrain Loss: 0.158511\n",
      "Train Epoch: 3 [6400/60000(11%)]\tTrain Loss: 0.230848\n",
      "Train Epoch: 3 [12800/60000(21%)]\tTrain Loss: 0.116123\n",
      "Train Epoch: 3 [19200/60000(32%)]\tTrain Loss: 0.302002\n",
      "Train Epoch: 3 [25600/60000(43%)]\tTrain Loss: 0.649862\n",
      "Train Epoch: 3 [32000/60000(53%)]\tTrain Loss: 0.415119\n",
      "Train Epoch: 3 [38400/60000(64%)]\tTrain Loss: 0.228562\n",
      "Train Epoch: 3 [44800/60000(75%)]\tTrain Loss: 0.129821\n",
      "Train Epoch: 3 [51200/60000(85%)]\tTrain Loss: 0.281156\n",
      "Train Epoch: 3 [57600/60000(96%)]\tTrain Loss: 0.548864\n",
      "\n",
      "EPOCH: 3], \tTest Loss: 0.0025, \tTest Accuracy: 97.54 %\n",
      "\n",
      "Train Epoch: 4 [0/60000(0%)]\tTrain Loss: 0.167259\n",
      "Train Epoch: 4 [6400/60000(11%)]\tTrain Loss: 0.335503\n",
      "Train Epoch: 4 [12800/60000(21%)]\tTrain Loss: 0.099947\n",
      "Train Epoch: 4 [19200/60000(32%)]\tTrain Loss: 0.359430\n",
      "Train Epoch: 4 [25600/60000(43%)]\tTrain Loss: 0.033980\n",
      "Train Epoch: 4 [32000/60000(53%)]\tTrain Loss: 0.126384\n",
      "Train Epoch: 4 [38400/60000(64%)]\tTrain Loss: 0.167336\n",
      "Train Epoch: 4 [44800/60000(75%)]\tTrain Loss: 0.281660\n",
      "Train Epoch: 4 [51200/60000(85%)]\tTrain Loss: 0.240326\n",
      "Train Epoch: 4 [57600/60000(96%)]\tTrain Loss: 0.313290\n",
      "\n",
      "EPOCH: 4], \tTest Loss: 0.0021, \tTest Accuracy: 97.94 %\n",
      "\n",
      "Train Epoch: 5 [0/60000(0%)]\tTrain Loss: 0.028091\n",
      "Train Epoch: 5 [6400/60000(11%)]\tTrain Loss: 0.543470\n",
      "Train Epoch: 5 [12800/60000(21%)]\tTrain Loss: 0.294191\n",
      "Train Epoch: 5 [19200/60000(32%)]\tTrain Loss: 0.295661\n",
      "Train Epoch: 5 [25600/60000(43%)]\tTrain Loss: 0.058312\n",
      "Train Epoch: 5 [32000/60000(53%)]\tTrain Loss: 0.448612\n",
      "Train Epoch: 5 [38400/60000(64%)]\tTrain Loss: 0.195858\n",
      "Train Epoch: 5 [44800/60000(75%)]\tTrain Loss: 0.349060\n",
      "Train Epoch: 5 [51200/60000(85%)]\tTrain Loss: 0.116766\n",
      "Train Epoch: 5 [57600/60000(96%)]\tTrain Loss: 0.114662\n",
      "\n",
      "EPOCH: 5], \tTest Loss: 0.0020, \tTest Accuracy: 98.03 %\n",
      "\n",
      "Train Epoch: 6 [0/60000(0%)]\tTrain Loss: 0.182675\n",
      "Train Epoch: 6 [6400/60000(11%)]\tTrain Loss: 0.173404\n",
      "Train Epoch: 6 [12800/60000(21%)]\tTrain Loss: 0.370670\n",
      "Train Epoch: 6 [19200/60000(32%)]\tTrain Loss: 0.245664\n",
      "Train Epoch: 6 [25600/60000(43%)]\tTrain Loss: 0.202054\n",
      "Train Epoch: 6 [32000/60000(53%)]\tTrain Loss: 0.140227\n",
      "Train Epoch: 6 [38400/60000(64%)]\tTrain Loss: 0.545718\n",
      "Train Epoch: 6 [44800/60000(75%)]\tTrain Loss: 0.578770\n",
      "Train Epoch: 6 [51200/60000(85%)]\tTrain Loss: 0.074094\n",
      "Train Epoch: 6 [57600/60000(96%)]\tTrain Loss: 0.469585\n",
      "\n",
      "EPOCH: 6], \tTest Loss: 0.0019, \tTest Accuracy: 98.13 %\n",
      "\n",
      "Train Epoch: 7 [0/60000(0%)]\tTrain Loss: 0.106687\n",
      "Train Epoch: 7 [6400/60000(11%)]\tTrain Loss: 0.044816\n",
      "Train Epoch: 7 [12800/60000(21%)]\tTrain Loss: 0.285524\n",
      "Train Epoch: 7 [19200/60000(32%)]\tTrain Loss: 0.149368\n",
      "Train Epoch: 7 [25600/60000(43%)]\tTrain Loss: 0.153554\n",
      "Train Epoch: 7 [32000/60000(53%)]\tTrain Loss: 0.283762\n",
      "Train Epoch: 7 [38400/60000(64%)]\tTrain Loss: 0.012164\n",
      "Train Epoch: 7 [44800/60000(75%)]\tTrain Loss: 0.169681\n",
      "Train Epoch: 7 [51200/60000(85%)]\tTrain Loss: 0.169927\n",
      "Train Epoch: 7 [57600/60000(96%)]\tTrain Loss: 0.040454\n",
      "\n",
      "EPOCH: 7], \tTest Loss: 0.0017, \tTest Accuracy: 98.31 %\n",
      "\n",
      "Train Epoch: 8 [0/60000(0%)]\tTrain Loss: 0.288238\n",
      "Train Epoch: 8 [6400/60000(11%)]\tTrain Loss: 0.086195\n",
      "Train Epoch: 8 [12800/60000(21%)]\tTrain Loss: 0.019466\n",
      "Train Epoch: 8 [19200/60000(32%)]\tTrain Loss: 0.177758\n",
      "Train Epoch: 8 [25600/60000(43%)]\tTrain Loss: 0.114668\n",
      "Train Epoch: 8 [32000/60000(53%)]\tTrain Loss: 0.028549\n",
      "Train Epoch: 8 [38400/60000(64%)]\tTrain Loss: 0.024322\n",
      "Train Epoch: 8 [44800/60000(75%)]\tTrain Loss: 0.244465\n",
      "Train Epoch: 8 [51200/60000(85%)]\tTrain Loss: 0.067673\n",
      "Train Epoch: 8 [57600/60000(96%)]\tTrain Loss: 0.091771\n",
      "\n",
      "EPOCH: 8], \tTest Loss: 0.0015, \tTest Accuracy: 98.52 %\n",
      "\n",
      "Train Epoch: 9 [0/60000(0%)]\tTrain Loss: 0.260233\n",
      "Train Epoch: 9 [6400/60000(11%)]\tTrain Loss: 0.027797\n",
      "Train Epoch: 9 [12800/60000(21%)]\tTrain Loss: 0.035021\n",
      "Train Epoch: 9 [19200/60000(32%)]\tTrain Loss: 0.098303\n",
      "Train Epoch: 9 [25600/60000(43%)]\tTrain Loss: 0.189110\n",
      "Train Epoch: 9 [32000/60000(53%)]\tTrain Loss: 0.259478\n",
      "Train Epoch: 9 [38400/60000(64%)]\tTrain Loss: 0.176106\n",
      "Train Epoch: 9 [44800/60000(75%)]\tTrain Loss: 0.072171\n",
      "Train Epoch: 9 [51200/60000(85%)]\tTrain Loss: 0.064631\n",
      "Train Epoch: 9 [57600/60000(96%)]\tTrain Loss: 0.169744\n",
      "\n",
      "EPOCH: 9], \tTest Loss: 0.0013, \tTest Accuracy: 98.78 %\n",
      "\n",
      "Train Epoch: 10 [0/60000(0%)]\tTrain Loss: 0.221696\n",
      "Train Epoch: 10 [6400/60000(11%)]\tTrain Loss: 0.319222\n",
      "Train Epoch: 10 [12800/60000(21%)]\tTrain Loss: 0.073997\n",
      "Train Epoch: 10 [19200/60000(32%)]\tTrain Loss: 0.009514\n",
      "Train Epoch: 10 [25600/60000(43%)]\tTrain Loss: 0.147278\n",
      "Train Epoch: 10 [32000/60000(53%)]\tTrain Loss: 0.117230\n",
      "Train Epoch: 10 [38400/60000(64%)]\tTrain Loss: 0.318296\n",
      "Train Epoch: 10 [44800/60000(75%)]\tTrain Loss: 0.061864\n",
      "Train Epoch: 10 [51200/60000(85%)]\tTrain Loss: 0.116688\n",
      "Train Epoch: 10 [57600/60000(96%)]\tTrain Loss: 0.072063\n",
      "\n",
      "EPOCH: 10], \tTest Loss: 0.0013, \tTest Accuracy: 98.69 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net_with_dropout_relu_BN(nn.Module): # nn.Module 클래스 상속 -> 딥러닝 모델 관련 기본 함수를 포함\n",
    "    def __init__(self): #  인스턴스 생성 시 지니게 되는 성질을 정의해주는 메서드\n",
    "        super(Net_with_dropout_relu_BN, self).__init__() # nn.Module 내에 있는 매서드를 상속받아 이용\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # 첫 번째 Fully Connected Layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256) # 두 번째 Fully Connected Layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10) # 세 번째 Fully Connected Layer 정의\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # 첫 번째 FC layer의 output 크기와 동일하게\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # 두 번째 FC layer의 output 크기와 동일하게\n",
    "    def forward(self, x): # Forward Propagation 정의\n",
    "        x = x.view(-1, 28 * 28) # 2차원 데이터를 1차원 데이터로 변환 -> MLP 모델은 1차원의 벡터 값을 입력으로 받을 수 있음\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x) # 활성화 함수\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1) # 확률 값 계산. log_softmax를 사용하는 이유는 gradient 값을 좀 더 원활하게 계산하기 위해 \n",
    "        return x\n",
    "\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear): # MLP 모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "        init.kaiming_uniform(m.weight.data) # 파라미터 값 초기화\n",
    "\n",
    "model = Net_with_dropout_relu_BN().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for Epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\nEPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n\".format(Epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
